{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protocollo Vectorization into Chars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import string\n",
    "\n",
    "random.seed(12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebooks used `string.printable` chars as basis for the vectorization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ \\t\\n\\r\\x0b\\x0c'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.printable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_filename = './protocollo_2017.csv'\n",
    "output_filename = './protocollo_2017_vectorized_chars.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(input_path, separator=\",\", contains_header=False):\n",
    "    with open(input_filename, 'r') as f:\n",
    "        lines = f.read().splitlines()\n",
    "    \n",
    "    if contains_header:\n",
    "        headers = lines[0].split(separator)\n",
    "        lines = lines[1:]\n",
    "    else:\n",
    "        headers = []\n",
    "    \n",
    "    data = [l.split(separator) for l in lines]\n",
    "    return headers, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers, data = read_data(input_filename, separator=\"|\", contains_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ANNO', 'NUMERO', 'OGGETTO', 'CLASSIFICA']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['2017', '6', '\"agli APE 2016_12_28_02267130488_006\"', '\"P\"'],\n",
       " ['2017', '6', '\"ape condominio via pa\"', '\"P\"'],\n",
       " ['2017', '6', '\"APE 2016_12_28_02267130488_006\"', '\"P\"'],\n",
       " ['2017', '6', '\"APE 2016_12_28_02267130488_006\"', '\"P\"'],\n",
       " ['2017', '6', '\"APE 2016_12_28_02267130488_006\"', '\"P\"']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to process the `OGGETTO` column\n",
    "\n",
    "## Preprocessing\n",
    "\n",
    "- filters: list (or concatenation) of characters to filter out, such as punctuation. Default: '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n' , includes basic punctuation, tabs, and newlines. These are replaced with spaces.\n",
    "- split by space\n",
    "- replace words with integers\n",
    "- save the new dataset with id and the dictionary used for the mapping\n",
    "\n",
    "Define the list of transformation to apply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text,\n",
    "               filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "               lower=True, split = ' '):\n",
    "    \"\"\"apply the transformations to the given text.\n",
    "\n",
    "    # Arguments\n",
    "        text: Input text (string).\n",
    "        filters: Sequence of characters to filter out.\n",
    "        lower: Whether to convert the input to lowercase.\n",
    "        split: Sentence split marker (string).\n",
    "        \n",
    "        return a string of token separated by split\n",
    "    \"\"\"\n",
    "    translate_map = str.maketrans(filters, split * len(filters))\n",
    "    text = text.translate(translate_map)\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(data, column=2):\n",
    "    data_processed = []\n",
    "    for v in data:\n",
    "        v[column] = preprocess(v[column])\n",
    "        data_processed.append(v)\n",
    "    return data_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['2017', '6', 'agli APE 2016 12 28 02267130488 006', 'P'],\n",
       " ['2017', '6', 'ape condominio via pa', 'P'],\n",
       " ['2017', '6', 'APE 2016 12 28 02267130488 006', 'P'],\n",
       " ['2017', '6', 'APE 2016 12 28 02267130488 006', 'P'],\n",
       " ['2017', '6', 'APE 2016 12 28 02267130488 006', 'P']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_processed = preprocess_dataset(data, column=2)\n",
    "data_processed = preprocess_dataset(data, column=3)\n",
    "data_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create the dictionary used for the token mapping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_chars_dict(data, column=2):\n",
    "    chars = [ch for ch in string.printable]\n",
    "    random.shuffle(chars)\n",
    "    \n",
    "    index_to_token = dict(enumerate(chars))\n",
    "    index_to_token[len(index_to_token)] = 'UNK'\n",
    "    token_to_index = {v:k for k,v in index_to_token.items()}\n",
    "    return index_to_token, token_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_chars, chars_to_index = extract_chars_dict(data_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\"', 1: 'j', 2: '@', 3: '^', 4: '_', 5: 's', 6: '8', 7: '/', 8: 'K', 9: 'X', 10: 'D', 11: '{', 12: 't', 13: 'Y', 14: '[', 15: 'u', 16: 'c', 17: 'p', 18: 'S', 19: 'h', 20: '!', 21: 'z', 22: 'r', 23: 'U', 24: '>', 25: '5', 26: '\\r', 27: '4', 28: '\\x0b', 29: 'E', 30: 'O', 31: 'g', 32: '<', 33: 'd', 34: 'v', 35: '7', 36: 'M', 37: '2', 38: ',', 39: '\\n', 40: ']', 41: 'e', 42: '(', 43: 'P', 44: 'A', 45: '.', 46: '\\x0c', 47: '$', 48: ':', 49: '6', 50: '}', 51: '?', 52: 'B', 53: 'N', 54: 'a', 55: 'G', 56: 'V', 57: '0', 58: '\\\\', 59: '`', 60: 'I', 61: 'w', 62: 'Z', 63: '%', 64: \"'\", 65: 'W', 66: '3', 67: 'F', 68: 'H', 69: ' ', 70: '9', 71: 'q', 72: 'i', 73: 'l', 74: '#', 75: '-', 76: 'Q', 77: '&', 78: 'b', 79: 'J', 80: 'n', 81: ')', 82: ';', 83: 'm', 84: '=', 85: '*', 86: 'x', 87: '|', 88: 'f', 89: '\\t', 90: 'k', 91: 'T', 92: '+', 93: 'y', 94: 'o', 95: 'L', 96: 'C', 97: '1', 98: '~', 99: 'R', 100: 'UNK'}\n",
      "{'\"': 0, 'j': 1, '@': 2, '^': 3, '_': 4, 's': 5, '8': 6, '/': 7, 'K': 8, 'X': 9, 'D': 10, '{': 11, 't': 12, 'Y': 13, '[': 14, 'u': 15, 'c': 16, 'p': 17, 'S': 18, 'h': 19, '!': 20, 'z': 21, 'r': 22, 'U': 23, '>': 24, '5': 25, '\\r': 26, '4': 27, '\\x0b': 28, 'E': 29, 'O': 30, 'g': 31, '<': 32, 'd': 33, 'v': 34, '7': 35, 'M': 36, '2': 37, ',': 38, '\\n': 39, ']': 40, 'e': 41, '(': 42, 'P': 43, 'A': 44, '.': 45, '\\x0c': 46, '$': 47, ':': 48, '6': 49, '}': 50, '?': 51, 'B': 52, 'N': 53, 'a': 54, 'G': 55, 'V': 56, '0': 57, '\\\\': 58, '`': 59, 'I': 60, 'w': 61, 'Z': 62, '%': 63, \"'\": 64, 'W': 65, '3': 66, 'F': 67, 'H': 68, ' ': 69, '9': 70, 'q': 71, 'i': 72, 'l': 73, '#': 74, '-': 75, 'Q': 76, '&': 77, 'b': 78, 'J': 79, 'n': 80, ')': 81, ';': 82, 'm': 83, '=': 84, '*': 85, 'x': 86, '|': 87, 'f': 88, '\\t': 89, 'k': 90, 'T': 91, '+': 92, 'y': 93, 'o': 94, 'L': 95, 'C': 96, '1': 97, '~': 98, 'R': 99, 'UNK': 100}\n"
     ]
    }
   ],
   "source": [
    "print(index_to_chars)\n",
    "print(chars_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the dictionary mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('index_to_chars.json','w') as f:\n",
    "    json.dump(index_to_chars,f)\n",
    "    \n",
    "with open('chars_to_index.json', 'w') as f:\n",
    "    json.dump(chars_to_index,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(chars_to_index, data, column=2):\n",
    "    data_transformed = []\n",
    "\n",
    "    for row in data:\n",
    "        transformed = [chars_to_index[v] for v in row[column] if v in chars_to_index]\n",
    "        row[column] = ' '.join([str(x) for x in transformed])\n",
    "        data_transformed.append(row)\n",
    "    \n",
    "    return data_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transformed = vectorize(chars_to_index, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['2017',\n",
       "  '6',\n",
       "  '54 31 73 72 69 44 43 29 69 37 57 97 49 69 97 37 69 37 6 69 57 37 37 49 35 97 66 57 27 6 6 69 57 57 49',\n",
       "  'P'],\n",
       " ['2017',\n",
       "  '6',\n",
       "  '54 17 41 69 16 94 80 33 94 83 72 80 72 94 69 34 72 54 69 17 54',\n",
       "  'P'],\n",
       " ['2017',\n",
       "  '6',\n",
       "  '44 43 29 69 37 57 97 49 69 97 37 69 37 6 69 57 37 37 49 35 97 66 57 27 6 6 69 57 57 49',\n",
       "  'P'],\n",
       " ['2017',\n",
       "  '6',\n",
       "  '44 43 29 69 37 57 97 49 69 97 37 69 37 6 69 57 37 37 49 35 97 66 57 27 6 6 69 57 57 49',\n",
       "  'P'],\n",
       " ['2017',\n",
       "  '6',\n",
       "  '44 43 29 69 37 57 97 49 69 97 37 69 37 6 69 57 37 37 49 35 97 66 57 27 6 6 69 57 57 49',\n",
       "  'P']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_filename, 'w') as f:\n",
    "    for line in data_transformed:\n",
    "        f.write(','.join(line) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
