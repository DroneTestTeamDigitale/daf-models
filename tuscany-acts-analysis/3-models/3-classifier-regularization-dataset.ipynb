{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/fabio/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/fabio/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import activations\n",
    "from keras import metrics\n",
    "from keras import regularizers\n",
    "import math\n",
    "\n",
    "import random\n",
    "\n",
    "from dataset import atti_dirigenti\n",
    "\n",
    "# to make the experimens replicable\n",
    "random.seed(123456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_val, y_val), (x_test, y_test) = atti_dirigenti.load_data(num_words=10000, remove_stopwords=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_index = atti_dirigenti.get_labels()\n",
    "len(label_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data\n",
    "\n",
    "for data and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension), dtype=np.float32)\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max dimension 10134\n"
     ]
    }
   ],
   "source": [
    "dimension = max(x_train.max() + x_val.max() + x_test.max()) + 10\n",
    "print('max dimension {}'.format(dimension))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = vectorize_sequences(x_train, dimension)\n",
    "x_val = vectorize_sequences(x_val, dimension)\n",
    "x_test = vectorize_sequences(x_test, dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99390, 10134)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoding for the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(labels):\n",
    "    results = np.zeros((len(labels), len(set(labels))), dtype=np.float16)\n",
    "    for i, label in enumerate(labels):\n",
    "        results[i, label] = 1.\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_one_hot(y_train)\n",
    "y_val = to_one_hot(y_val)\n",
    "y_test = to_one_hot(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float16)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Models\n",
    "\n",
    "apart the base model we add new methods with different kind of regularizers. In particular, we take into account:\n",
    "- l2 that penalize the weights coefficient with a value proportional of their l1 and l2 norms. The general idea is that we prefer a simple model where the distribution of parameters values has less entropy.\n",
    "- dropout where the idea is to reset randomly a percentage of the weights to zero in order to avoid that neurons start to memorize noise patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(neurons):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(neurons, activation='relu', input_shape=(x_train.shape[-1], )))\n",
    "    model.add(layers.Dense(neurons, activation='relu'))\n",
    "    model.add(layers.Dense(neurons, activation='relu'))\n",
    "    model.add(layers.Dense(len(label_index), activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer=optimizers.Adam(), \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_l2(neurons):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(neurons, activation='relu', kernel_regularizer=regularizers.l2(), input_shape=(x_train.shape[-1], )))\n",
    "    model.add(layers.Dense(neurons, kernel_regularizer=regularizers.l2(0.0001), activation='relu'))\n",
    "    model.add(layers.Dense(neurons, kernel_regularizer=regularizers.l2(0.0001), activation='relu'))\n",
    "    model.add(layers.Dense(len(label_index), activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer=optimizers.Adam(), \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_dropout(neurons, dropout= 0.1):\n",
    "    first_layer = True\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    for neuron in neurons:\n",
    "        if first_layer:\n",
    "            model.add(layers.Dense(neuron, activation='relu', input_shape=(x_train.shape[-1], )))\n",
    "            model.add(layers.Dropout(dropout))\n",
    "            first_layer = False\n",
    "        else:\n",
    "            model.add(layers.Dense(neuron, activation='relu'))\n",
    "            model.add(layers.Dropout(dropout))\n",
    "            \n",
    "    model.add(layers.Dense(len(label_index), activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer=optimizers.Adam(), \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base = build_model(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 99390 samples, validate on 11044 samples\n",
      "Epoch 1/10\n",
      "99390/99390 [==============================] - 6s 59us/step - loss: 0.8533 - acc: 0.7238 - val_loss: 0.5489 - val_acc: 0.8118\n",
      "Epoch 2/10\n",
      "99390/99390 [==============================] - 4s 43us/step - loss: 0.3684 - acc: 0.8695 - val_loss: 0.5115 - val_acc: 0.8264\n",
      "Epoch 3/10\n",
      "99390/99390 [==============================] - 4s 43us/step - loss: 0.2404 - acc: 0.9133 - val_loss: 0.5301 - val_acc: 0.8314\n",
      "Epoch 4/10\n",
      "99390/99390 [==============================] - 4s 45us/step - loss: 0.1637 - acc: 0.9420 - val_loss: 0.5772 - val_acc: 0.8367\n",
      "Epoch 5/10\n",
      "99390/99390 [==============================] - 4s 44us/step - loss: 0.1190 - acc: 0.9596 - val_loss: 0.6353 - val_acc: 0.8343\n",
      "Epoch 6/10\n",
      "99390/99390 [==============================] - 4s 44us/step - loss: 0.0927 - acc: 0.9689 - val_loss: 0.6840 - val_acc: 0.8305\n",
      "Epoch 7/10\n",
      "99390/99390 [==============================] - 4s 44us/step - loss: 0.0768 - acc: 0.9744 - val_loss: 0.7085 - val_acc: 0.8329\n",
      "Epoch 8/10\n",
      "99390/99390 [==============================] - 4s 43us/step - loss: 0.0652 - acc: 0.9781 - val_loss: 0.7661 - val_acc: 0.8327\n",
      "Epoch 9/10\n",
      "99390/99390 [==============================] - 4s 44us/step - loss: 0.0569 - acc: 0.9808 - val_loss: 0.7883 - val_acc: 0.8372\n",
      "Epoch 10/10\n",
      "99390/99390 [==============================] - 4s 44us/step - loss: 0.0534 - acc: 0.9817 - val_loss: 0.8238 - val_acc: 0.8367\n"
     ]
    }
   ],
   "source": [
    "history_base = model_base.fit(x=x_train, y=y_train, validation_data=(x_val, y_val),\n",
    "                   epochs=10, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_l2 = build_model_l2(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 99390 samples, validate on 11044 samples\n",
      "Epoch 1/10\n",
      "99390/99390 [==============================] - 6s 59us/step - loss: 1.6742 - acc: 0.6559 - val_loss: 1.3778 - val_acc: 0.7285\n",
      "Epoch 2/10\n",
      "99390/99390 [==============================] - 6s 56us/step - loss: 1.3472 - acc: 0.7394 - val_loss: 1.3240 - val_acc: 0.7524\n",
      "Epoch 3/10\n",
      "99390/99390 [==============================] - 6s 57us/step - loss: 1.2856 - acc: 0.7616 - val_loss: 1.2620 - val_acc: 0.7618\n",
      "Epoch 4/10\n",
      "99390/99390 [==============================] - 6s 56us/step - loss: 1.2226 - acc: 0.7775 - val_loss: 1.2320 - val_acc: 0.7709\n",
      "Epoch 5/10\n",
      "99390/99390 [==============================] - 6s 58us/step - loss: 1.1673 - acc: 0.7891 - val_loss: 1.1754 - val_acc: 0.7847\n",
      "Epoch 6/10\n",
      "99390/99390 [==============================] - 6s 58us/step - loss: 1.1103 - acc: 0.7995 - val_loss: 1.1640 - val_acc: 0.7852\n",
      "Epoch 7/10\n",
      "99390/99390 [==============================] - 6s 56us/step - loss: 1.0708 - acc: 0.8056 - val_loss: 1.1148 - val_acc: 0.7861\n",
      "Epoch 8/10\n",
      "99390/99390 [==============================] - 6s 59us/step - loss: 1.0312 - acc: 0.8118 - val_loss: 1.1003 - val_acc: 0.7895\n",
      "Epoch 9/10\n",
      "99390/99390 [==============================] - 6s 58us/step - loss: 1.0039 - acc: 0.8158 - val_loss: 1.0881 - val_acc: 0.7941\n",
      "Epoch 10/10\n",
      "99390/99390 [==============================] - 6s 59us/step - loss: 0.9863 - acc: 0.8190 - val_loss: 1.0639 - val_acc: 0.7954\n"
     ]
    }
   ],
   "source": [
    "history_l2 = model_l2.fit(x=x_train, y=y_train, validation_data=(x_val, y_val),\n",
    "                   epochs=10, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dropout = build_model_dropout(neurons = [256, 128], dropout = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 99390 samples, validate on 11044 samples\n",
      "Epoch 1/5\n",
      "99390/99390 [==============================] - 5s 47us/step - loss: 0.2074 - acc: 0.9277 - val_loss: 0.5160 - val_acc: 0.8428\n",
      "Epoch 2/5\n",
      "99390/99390 [==============================] - 5s 46us/step - loss: 0.1813 - acc: 0.9364 - val_loss: 0.5401 - val_acc: 0.8421\n",
      "Epoch 3/5\n",
      "99390/99390 [==============================] - 5s 46us/step - loss: 0.1562 - acc: 0.9458 - val_loss: 0.5580 - val_acc: 0.8450\n",
      "Epoch 4/5\n",
      "99390/99390 [==============================] - 5s 46us/step - loss: 0.1398 - acc: 0.9515 - val_loss: 0.5808 - val_acc: 0.8460\n",
      "Epoch 5/5\n",
      "99390/99390 [==============================] - 5s 45us/step - loss: 0.1271 - acc: 0.9563 - val_loss: 0.6071 - val_acc: 0.8454\n"
     ]
    }
   ],
   "source": [
    "history_dropout = model_dropout.fit(x=x_train, y=y_train, validation_data=(x_val, y_val),\n",
    "                   epochs=5, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing The Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chart_loss(history, name):\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "\n",
    "    plt.plot(epochs, loss, 'b+', label='Training Loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
    "    plt.title('Training and validation loss {}'.format(name))\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.yticks(np.arange(0,2.2, step=0.2))\n",
    "    plt.xticks(epochs)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_loss(history_base, 'Base')\n",
    "chart_loss(history_l2, 'L2')\n",
    "chart_loss(history_dropout, 'Dropout')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the charts we can see that: \n",
    "- the model with l2 regularization is able to avoid overfitting during the training. \n",
    "- the model that uses dropout (0.5) has a lower loss with respect to l2 while combatting overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chart_acc(history, name):\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.plot(epochs, acc, 'b+', label='Training Acc')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation Acc')\n",
    "    plt.title('Training and validation acc {}'.format(name))\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuray')\n",
    "    plt.yticks(np.arange(0.5,1.05, step=0.05))\n",
    "    plt.xticks(epochs)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_acc(history_base, 'Base')\n",
    "chart_acc(history_l2, 'L2')\n",
    "chart_acc(history_dropout, 'Dropout')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the chart above we can see that the best model is the model that uses dropout, while the best epoch is the 6 where the validation accuracy crosses the training accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_loss(histories):\n",
    "    epochs = range(1, len(list(histories.values())[0].history['val_loss']) + 1)\n",
    "\n",
    "    for i, history in histories.items():\n",
    "        val_loss = history.history['val_loss']\n",
    "        plt.plot(epochs, val_loss, label='Validation Loss {}'.format(i))\n",
    "            \n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_loss({'Base': history_base, 'L2': history_l2, 'Dropout': history_dropout})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_accuracy(histories):\n",
    "    epochs = range(1, len(list(histories.values())[0].history['val_acc']) + 1)\n",
    "\n",
    "    for i, history in histories.items():\n",
    "        val_loss = history.history['val_acc']\n",
    "        plt.plot(epochs, val_loss, label='Validation Accuracy {}'.format(i))\n",
    "            \n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_accuracy({'Base': history_base, 'L2': history_l2, 'Dropout': history_dropout})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_loss(history):\n",
    "    val_loss = history.history['val_loss'] \n",
    "    return np.argmin(val_loss) + 1   \n",
    "\n",
    "def accuracy(history, epoch):\n",
    "    val_acc = history.history['val_acc']\n",
    "    return val_acc[epoch-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('min loss for model base is {}'.format(min_loss(history_base)))\n",
    "print('min loss for model L2 is {}'.format(min_loss(history_l2)))\n",
    "print('min loss for model Dropout is {}'.format(min_loss(history_dropout)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('best validation accuracy for model base {}'.format(\n",
    "    accuracy(history_base, min_loss(history_base))))\n",
    "print('best validation accuracy for model L2 {}'.format(\n",
    "    accuracy(history_l2, min_loss(history_l2))))\n",
    "print('best validation accuracy for model Dropout {}'.format(\n",
    "    accuracy(history_dropout, min_loss(history_dropout))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on the Test Set\n",
    "\n",
    "- train the best model for the best epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model_dropout(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=np.concatenate([x_train, x_val]), y=np.concatenate([y_train, y_val]), epochs=6, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('loss {}'.format(loss))\n",
    "print('acc {}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "As recap we can see that using regularization, in particular dropout, the accuracy grows from 0.83 to 0.84."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
