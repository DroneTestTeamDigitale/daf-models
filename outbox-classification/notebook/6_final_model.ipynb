{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabio/miniconda3/envs/dl/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import *\n",
    "from sklearn import metrics\n",
    "\n",
    "from daf.datasets import atti_dataset\n",
    "from daf.utils import dataset_utils\n",
    "from daf.utils import keras_util\n",
    "import keras\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34731"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = atti_dataset.load_data(num_words=None)\n",
    "label_index_dict = atti_dataset.get_label_index()\n",
    "\n",
    "num_words = max([max(x) for x in x_train]) + 1\n",
    "num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ALTRI UFFICI': 0,\n",
       " 'AVVOCATURA REGIONALE                                  ': 1,\n",
       " 'D.G.  AVVOCATURA                                      ': 2,\n",
       " \"D.G. COMPETITIVITA' DEL SISTEMA REGIONALE E SVILUPPO D\": 3,\n",
       " 'D.G. PRESIDENZA                                       ': 4,\n",
       " 'DIPARTIMENTO BILANCIO E FINANZE                       ': 5,\n",
       " 'DIPARTIMENTO ORGANIZZAZIONE                           ': 6,\n",
       " 'DIPARTIMENTO ORGANIZZAZIONE E RISORSE                 ': 7,\n",
       " 'DIPARTIMENTO POLITICHE FORMATIVE E BENI CULTURALI     ': 8,\n",
       " 'DIPARTIMENTO POLITICHE TERRITORIALI E AMBIENTALI      ': 9,\n",
       " 'DIPARTIMENTO PRESIDENZA AFFARI LEGISLATIVI E GIURIDICI': 10,\n",
       " 'DIPARTIMENTO SALUTE E POLITICHE SOLIDARIETA           ': 11,\n",
       " 'DIPARTIMENTO SVILUPPO ECONOMICO                       ': 12,\n",
       " 'DIREZIONE AGRICOLTURA E SVILUPPO RURALE': 13,\n",
       " \"DIREZIONE ATTIVITA' PRODUTTIVE\": 14,\n",
       " 'DIREZIONE DIFESA DEL SUOLO E PROTEZIONE CIVILE': 15,\n",
       " 'DIREZIONE DIRITTI DI CITTADINANZA E COESIONE SOCIALE': 16,\n",
       " 'DIREZIONE GENERALE BILANCIO E FINANZE                 ': 17,\n",
       " 'DIREZIONE GENERALE DIRITTO ALLA SALUTE E POLITICHE DI ': 18,\n",
       " 'DIREZIONE GENERALE POLITICHE FORMATIVE, BENI E ATTIVIT': 19,\n",
       " \"DIREZIONE GENERALE POLITICHE MOBILITA', INFRASTRUTTURE E TRASPORTO PUBBLICO LOCALE\": 20,\n",
       " 'DIREZIONE GENERALE POLITICHE TERRITORIALI E AMBIENTALI': 21,\n",
       " 'DIREZIONE GENERALE SVILUPPO ECONOMICO                 ': 22,\n",
       " 'DIREZIONE ISTRUZIONE E FORMAZIONE': 23,\n",
       " 'DIREZIONE LAVORO': 24,\n",
       " 'DIREZIONE ORGANIZZAZIONE E SISTEMI INFORMATIVI': 25,\n",
       " 'POLITICHE AMBIENTALI, ENERGIA E CAMBIAMENTI CLIMATICI': 26,\n",
       " 'UFFICI DEL GENIO CIVILE                               ': 27}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.concatenate((x_train, x_test))\n",
    "y = np.concatenate((y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34731"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([1253, 1342, 460, 4, 397, 1405, 1221, 28, 1150, 824, 35, 54, 2, 20, 8769]),\n",
       "       list([3563, 3, 33, 506, 2826, 112, 16, 2105, 4, 65, 20, 47, 84, 2, 3]),\n",
       "       list([93, 7, 60, 8, 19, 814, 6, 990, 7, 620, 818, 110, 13492, 2486, 5]),\n",
       "       ...,\n",
       "       list([41, 21, 15, 3984, 17, 85, 18388, 3, 33, 66, 200, 4893, 4, 826, 3, 81, 371]),\n",
       "       list([1253, 2, 2, 4, 397, 1405, 1221, 28, 1150, 824, 35, 54, 22340, 20, 8769]),\n",
       "       list([109, 8, 397, 6, 140, 3642, 28, 6979, 2, 163, 9, 31, 42, 18, 2, 9, 80, 63, 9, 11, 7904, 9, 781, 9, 8, 275, 517, 11, 709, 507, 256, 248, 232, 15, 248, 1132, 4, 755, 6, 2, 110, 1973, 11, 3202, 7, 2486, 17, 2])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 34731)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_transformer(x_train[0])().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def x_transformer(x_data):\n",
    "    return partial(dataset_utils.vectorize_sequences, x_data, num_words)\n",
    "\n",
    "def y_tranformer(y_data):\n",
    "    return partial(dataset_utils.to_one_hot, y_data, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dropout_model(neurons, num_words, num_classes, dropout=0.5):\n",
    "    input_l = keras.Input(shape=(num_words, ))\n",
    "    l = keras.layers.Dense(neurons, activation='relu')(input_l)\n",
    "    l = keras.layers.Dropout(dropout)(l)\n",
    "    l = keras.layers.Dense(neurons, activation='relu')(l)\n",
    "    l = keras.layers.Dropout(dropout)(l)\n",
    "    l = keras.layers.Dense(neurons, activation='relu')(l)\n",
    "    output_l = keras.layers.Dense(num_classes, activation='softmax')(l)\n",
    "    model = keras.Model(inputs=input_l, outputs=output_l)\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps = len(x_train) // batch_size + 1\n",
    "\n",
    "train_generator = dataset_utils.dataset_generator_fun(x_train, y_train, x_transformer, y_tranformer, \n",
    "                                                      batch_size, 0, len(x_train), True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 34731)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               17782784  \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 28)                14364     \n",
      "=================================================================\n",
      "Total params: 18,322,460\n",
      "Trainable params: 18,322,460\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_dropout_model(512, num_words, num_classes)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../checkpoints/weights.{epoch:02d}-{loss:.2f}.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(model_path, verbose=1, save_best_only=True, monitor='loss'),\n",
    "    keras.callbacks.ReduceLROnPlateau(patience=2, verbose=1, monitor='loss')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "955/955 [==============================] - 35s 36ms/step - loss: 0.8795 - acc: 0.7229\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.87973, saving model to ../checkpoints/weights.01-0.88.hdf5\n",
      "Epoch 2/3\n",
      "955/955 [==============================] - 34s 36ms/step - loss: 0.4417 - acc: 0.8565\n",
      "\n",
      "Epoch 00002: loss improved from 0.87973 to 0.44180, saving model to ../checkpoints/weights.02-0.44.hdf5\n",
      "Epoch 3/3\n",
      "955/955 [==============================] - 34s 35ms/step - loss: 0.3302 - acc: 0.8923\n",
      "\n",
      "Epoch 00003: loss improved from 0.44180 to 0.33017, saving model to ../checkpoints/weights.03-0.33.hdf5\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator, train_steps, 3, callbacks=train_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../checkpoints/final_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('../checkpoints/final_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = x_transformer(x_train[:10])()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = model.predict_on_batch(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(model.predict(d)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4, 14,  9, 15, 16,  3, 22, 13, 16, 22])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/fabio/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/fabio/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = ['-', '\"', \"'\", ':', ';', '(', ')', '[', ']', '{', '}', '’', '”', '“', '``', \"''\"]\n",
    "stop_words = set(stopwords.words('italian'))\n",
    "stop_words.update(punctuation)\n",
    "\n",
    "pad_char = 0\n",
    "start_char=1\n",
    "oov_char=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/dataset/id_word_dict.json', 'r') as f:\n",
    "    word_id_dict = json.load(f)\n",
    "\n",
    "with open('../data/dataset/label_index.json', 'r') as f:\n",
    "    label_id_dict = json.load(f)\n",
    "    id_label_dict = {v: k for k, v in label_id_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-862ee67cd261>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mtokenize_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \"\"\"\n\u001b[1;32m      7\u001b[0m     \u001b[0mTokenize\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mremove\u001b[0m \u001b[0mstopwords\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "def hasnumbers(value):\n",
    "    return any(c.isdigit() for c in value)\n",
    "\n",
    "\n",
    "def tokenize_sentence(sentence, remove_stopwords=False, tokenizer=tokenizer.tokenize):\n",
    "    \"\"\"\n",
    "    Tokenize the sentence and remove stopwords if true\n",
    "    :param sentence: the sentence to be tokenized\n",
    "    :param remove_stopwords: True to remove stopwa\n",
    "    :param tokenizer:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    sentence = sentence.replace('`', ' ')\n",
    "    sentence = sentence.replace(\"'\", \" \")\n",
    "    sentence = sentence.replace(\"”\", ' ')\n",
    "    sentence = sentence.replace(\"“\", ' ')\n",
    "    words = []\n",
    "\n",
    "    for w in tokenizer(sentence):\n",
    "        if not hasnumbers(w) and len(w) > 2:\n",
    "            w = w.replace('_', '')\n",
    "            if remove_stopwords:\n",
    "                if w not in stop_words:\n",
    "                    words.append(w.lower())\n",
    "            elif w in stop_words or len(w) > 1:\n",
    "                words.append(w.lower())\n",
    "    yield words\n",
    "\n",
    "\n",
    "def sentence_to_idxs(tokenized_sentence):\n",
    "    \"\"\"\n",
    "    convert a tokenized sentence into a sequence of idx\n",
    "    :param tokenized_sentence:\n",
    "    :param max_idx:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for sample in tokenized_sentence:\n",
    "        encoded_sample = []\n",
    "        for w in sample:\n",
    "            if w in word_id_dict:\n",
    "                encoded_sample.append(word_id_dict[w])\n",
    "            else:\n",
    "                encoded_sample.append(oov_char)\n",
    "        results.append(encoded_sample)\n",
    "    return results\n",
    "\n",
    "\n",
    "def vectorize_sequences(sequences, num_words):\n",
    "    \"\"\"\n",
    "\n",
    "    :param sequences:\n",
    "    :param dimension:\n",
    "    :return: sequences encoded as indicator arrays\n",
    "    \"\"\"\n",
    "    results = np.zeros((len(sequences), num_words))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "def sentence_pipeline(sentence):\n",
    "    \"\"\"\n",
    "    :param sentence:\n",
    "    :return: the sentence into its vectorized form\n",
    "    \"\"\"\n",
    "    tokenized = list(tokenize_sentence(sentence))\n",
    "    sequence = list(sentence_to_idxs(tokenized))\n",
    "    vectorized = vectorize_sequences([sequence], num_words)\n",
    "    return vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Attività riconosciuta. Corso  Adempimenti Amministrativi e contabili per la gestione aziendale matr. 2018GL0044 - Ag. CAT - ASCOM Maremma. Nomina Commissione di esame.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentence_pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-fc583d73fd7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvectorized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sentence_pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "vectorized = sentence_pipeline(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 34731)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(model.predict(vectorized)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
