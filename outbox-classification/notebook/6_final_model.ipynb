{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import *\n",
    "from sklearn import metrics\n",
    "\n",
    "from daf.datasets import atti_dataset\n",
    "from daf.utils import dataset_utils\n",
    "from daf.utils import keras_util\n",
    "import keras\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34731"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = atti_dataset.load_data(num_words=None)\n",
    "label_index_dict = atti_dataset.get_label_index()\n",
    "\n",
    "num_words = max([max(x) for x in x_train]) + 1\n",
    "num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ALTRI UFFICI': 0,\n",
       " 'AVVOCATURA REGIONALE                                  ': 1,\n",
       " 'D.G.  AVVOCATURA                                      ': 2,\n",
       " \"D.G. COMPETITIVITA' DEL SISTEMA REGIONALE E SVILUPPO D\": 3,\n",
       " 'D.G. PRESIDENZA                                       ': 4,\n",
       " 'DIPARTIMENTO BILANCIO E FINANZE                       ': 5,\n",
       " 'DIPARTIMENTO ORGANIZZAZIONE                           ': 6,\n",
       " 'DIPARTIMENTO ORGANIZZAZIONE E RISORSE                 ': 7,\n",
       " 'DIPARTIMENTO POLITICHE FORMATIVE E BENI CULTURALI     ': 8,\n",
       " 'DIPARTIMENTO POLITICHE TERRITORIALI E AMBIENTALI      ': 9,\n",
       " 'DIPARTIMENTO PRESIDENZA AFFARI LEGISLATIVI E GIURIDICI': 10,\n",
       " 'DIPARTIMENTO SALUTE E POLITICHE SOLIDARIETA           ': 11,\n",
       " 'DIPARTIMENTO SVILUPPO ECONOMICO                       ': 12,\n",
       " 'DIREZIONE AGRICOLTURA E SVILUPPO RURALE': 13,\n",
       " \"DIREZIONE ATTIVITA' PRODUTTIVE\": 14,\n",
       " 'DIREZIONE DIFESA DEL SUOLO E PROTEZIONE CIVILE': 15,\n",
       " 'DIREZIONE DIRITTI DI CITTADINANZA E COESIONE SOCIALE': 16,\n",
       " 'DIREZIONE GENERALE BILANCIO E FINANZE                 ': 17,\n",
       " 'DIREZIONE GENERALE DIRITTO ALLA SALUTE E POLITICHE DI ': 18,\n",
       " 'DIREZIONE GENERALE POLITICHE FORMATIVE, BENI E ATTIVIT': 19,\n",
       " \"DIREZIONE GENERALE POLITICHE MOBILITA', INFRASTRUTTURE E TRASPORTO PUBBLICO LOCALE\": 20,\n",
       " 'DIREZIONE GENERALE POLITICHE TERRITORIALI E AMBIENTALI': 21,\n",
       " 'DIREZIONE GENERALE SVILUPPO ECONOMICO                 ': 22,\n",
       " 'DIREZIONE ISTRUZIONE E FORMAZIONE': 23,\n",
       " 'DIREZIONE LAVORO': 24,\n",
       " 'DIREZIONE ORGANIZZAZIONE E SISTEMI INFORMATIVI': 25,\n",
       " 'POLITICHE AMBIENTALI, ENERGIA E CAMBIAMENTI CLIMATICI': 26,\n",
       " 'UFFICI DEL GENIO CIVILE                               ': 27}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.concatenate((x_train, x_test))\n",
    "y = np.concatenate((y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def x_transformer(x_data):\n",
    "    return partial(dataset_utils.vectorize_sequences, x_data, num_words)\n",
    "\n",
    "def y_tranformer(y_data):\n",
    "    return partial(dataset_utils.to_one_hot, y_data, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = len(set(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dropout_model(neurons, num_words, num_classes, dropout=0.5):\n",
    "    input_l = keras.Input(shape=(num_words, ))\n",
    "    l = keras.layers.Dense(neurons, activation='relu')(input_l)\n",
    "    l = keras.layers.Dropout(dropout)(l)\n",
    "    l = keras.layers.Dense(neurons, activation='relu')(l)\n",
    "    l = keras.layers.Dropout(dropout)(l)\n",
    "    l = keras.layers.Dense(neurons, activation='relu')(l)\n",
    "    output_l = keras.layers.Dense(num_classes, activation='softmax')(l)\n",
    "    model = keras.Model(inputs=input_l, outputs=output_l)\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps = len(x_train) // batch_size + 1\n",
    "\n",
    "train_generator = dataset_utils.dataset_generator_fun(x_train, y_train, x_transformer, y_tranformer, \n",
    "                                                      batch_size, 0, len(x_train), True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 34731)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               17782784  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 28)                14364     \n",
      "=================================================================\n",
      "Total params: 18,322,460\n",
      "Trainable params: 18,322,460\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_dropout_model(512, num_words, num_classes)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../checkpoints/weights.{epoch:02d}-{loss:.2f}.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(model_path, verbose=1, save_best_only=True, monitor='loss'),\n",
    "    keras.callbacks.ReduceLROnPlateau(patience=2, verbose=1, monitor='loss')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "1150/1151 [============================>.] - ETA: 0s - loss: 1.1579 - acc: 0.6346\n",
      "Epoch 00001: loss improved from inf to 1.15781, saving model to ../checkpoints/weights.01-1.16.hdf5\n",
      "1151/1151 [==============================] - 76s 66ms/step - loss: 1.1575 - acc: 0.6346\n",
      "Epoch 2/6\n",
      "1150/1151 [============================>.] - ETA: 0s - loss: 0.6924 - acc: 0.7693\n",
      "Epoch 00002: loss improved from 1.15781 to 0.69239, saving model to ../checkpoints/weights.02-0.69.hdf5\n",
      "1151/1151 [==============================] - 74s 65ms/step - loss: 0.6921 - acc: 0.7694\n",
      "Epoch 3/6\n",
      "1150/1151 [============================>.] - ETA: 0s - loss: 0.5504 - acc: 0.8165\n",
      "Epoch 00003: loss improved from 0.69239 to 0.55043, saving model to ../checkpoints/weights.03-0.55.hdf5\n",
      "1151/1151 [==============================] - 75s 66ms/step - loss: 0.5505 - acc: 0.8166\n",
      "Epoch 4/6\n",
      "1150/1151 [============================>.] - ETA: 0s - loss: 0.4703 - acc: 0.8423\n",
      "Epoch 00004: loss improved from 0.55043 to 0.47026, saving model to ../checkpoints/weights.04-0.47.hdf5\n",
      "1151/1151 [==============================] - 75s 65ms/step - loss: 0.4700 - acc: 0.8423\n",
      "Epoch 5/6\n",
      "1150/1151 [============================>.] - ETA: 0s - loss: 0.4127 - acc: 0.8618\n",
      "Epoch 00005: loss improved from 0.47026 to 0.41268, saving model to ../checkpoints/weights.05-0.41.hdf5\n",
      "1151/1151 [==============================] - 75s 65ms/step - loss: 0.4126 - acc: 0.8618\n",
      "Epoch 6/6\n",
      "1150/1151 [============================>.] - ETA: 0s - loss: 0.3733 - acc: 0.8744\n",
      "Epoch 00006: loss improved from 0.41268 to 0.37328, saving model to ../checkpoints/weights.06-0.37.hdf5\n",
      "1151/1151 [==============================] - 74s 65ms/step - loss: 0.3733 - acc: 0.8744\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator, train_steps, 6, callbacks=train_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../checkpoints/final_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('../checkpoints/final_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dataset_utils.vectorize_sequences(x_train[0], num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(model.predict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/fabio/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/fabio/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = ['-', '\"', \"'\", ':', ';', '(', ')', '[', ']', '{', '}', '’', '”', '“', '``', \"''\"]\n",
    "stop_words = set(stopwords.words('italian'))\n",
    "stop_words.update(punctuation)\n",
    "\n",
    "pad_char = 0\n",
    "start_char=1\n",
    "oov_char=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/dataset/id_word_dict.json', 'r') as f:\n",
    "    word_id_dict = json.load(f)\n",
    "\n",
    "with open('../data/dataset/label_index.json', 'r') as f:\n",
    "    label_id_dict = json.load(f)\n",
    "    id_label_dict = {v: k for k, v in label_id_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasnumbers(value):\n",
    "    return any(c.isdigit() for c in value)\n",
    "\n",
    "\n",
    "def tokenize_sentence(sentence, remove_stopwords=False, tokenizer=tokenizer.tokenize):\n",
    "    \"\"\"\n",
    "    Tokenize the sentence and remove stopwords if true\n",
    "    :param sentence: the sentence to be tokenized\n",
    "    :param remove_stopwords: True to remove stopwa\n",
    "    :param tokenizer:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    sentence = sentence.replace('`', ' ')\n",
    "    sentence = sentence.replace(\"'\", \" \")\n",
    "    sentence = sentence.replace(\"”\", ' ')\n",
    "    sentence = sentence.replace(\"“\", ' ')\n",
    "    words = []\n",
    "\n",
    "    for w in tokenizer(sentence):\n",
    "        if not hasnumbers(w) and len(w) > 2:\n",
    "            w = w.replace('_', '')\n",
    "            if remove_stopwords:\n",
    "                if w not in stop_words:\n",
    "                    words.append(w.lower())\n",
    "            elif w in stop_words or len(w) > 1:\n",
    "                words.append(w.lower())\n",
    "    yield words\n",
    "\n",
    "\n",
    "def sentence_to_idxs(tokenized_sentence):\n",
    "    \"\"\"\n",
    "    convert a tokenized sentence into a sequence of idx\n",
    "    :param tokenized_sentence:\n",
    "    :param max_idx:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for sample in tokenized_sentence:\n",
    "        encoded_sample = []\n",
    "        for w in sample:\n",
    "            if w in word_id_dict:\n",
    "                encoded_sample.append(word_id_dict[w])\n",
    "            else:\n",
    "                encoded_sample.append(oov_char)\n",
    "        results.append(encoded_sample)\n",
    "    return results\n",
    "\n",
    "\n",
    "def vectorize_sequences(sequences, num_words):\n",
    "    \"\"\"\n",
    "\n",
    "    :param sequences:\n",
    "    :param dimension:\n",
    "    :return: sequences encoded as indicator arrays\n",
    "    \"\"\"\n",
    "    results = np.zeros((len(sequences), num_words))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "def sentence_pipeline(sentence):\n",
    "    \"\"\"\n",
    "    :param sentence:\n",
    "    :return: the sentence into its vectorized form\n",
    "    \"\"\"\n",
    "    tokenized = list(tokenize_sentence(sentence))\n",
    "    sequence = list(sentence_to_idxs(tokenized))\n",
    "    vectorized = vectorize_sequences([sequence], num_words)\n",
    "    return vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'CONCESSIONE DI COLTIVAZIONE DI RISORSE GEOTERMICHE \"PIANCASTAGNAIO\" - AUTORIZZAZIONE REALIZZAZIONE MODIFICHE IMPIANTISTICHE PER ELIMINAZIONE TRASCINATO LIQUIDO IN TURBINA PRESSO LA CENTRALE GEOTERMOELETTRICA PIANCASTAGNAIO 5 - COMUNE DI PIANCASTAGNAIO (SI) - ART. 84 D.LGS. 624/96'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized = sentence_pipeline(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 52396)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(model.predict(vectorized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
