{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabio/miniconda3/envs/dl/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import *\n",
    "from sklearn import metrics\n",
    "\n",
    "from daf.datasets import atti_dataset\n",
    "from daf.utils import dataset_utils\n",
    "from daf.utils import keras_util\n",
    "import keras\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A local file was found, but it seems to be incomplete or outdated because the auto file hash does not match the original value of d6eaee8bd106697b93982053ec65aad1 so we will re-download the data.\n",
      "Downloading data from  https://media.githubusercontent.com/media/teamdigitale/daf-models/master/daf-datasets/data/atti/atti_dataset.npz\n",
      "6643712/6643618 [==============================] - 4s 1us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "34731"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = atti_dataset.load_data(num_words=None)\n",
    "label_index_dict = atti_dataset.get_label_index()\n",
    "\n",
    "num_words = max([max(x) for x in x_train]) + 1\n",
    "num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ALTRI UFFICI': 0,\n",
       " 'AVVOCATURA REGIONALE                                  ': 1,\n",
       " 'D.G.  AVVOCATURA                                      ': 2,\n",
       " \"D.G. COMPETITIVITA' DEL SISTEMA REGIONALE E SVILUPPO D\": 3,\n",
       " 'D.G. PRESIDENZA                                       ': 4,\n",
       " 'DIPARTIMENTO BILANCIO E FINANZE                       ': 5,\n",
       " 'DIPARTIMENTO ORGANIZZAZIONE                           ': 6,\n",
       " 'DIPARTIMENTO ORGANIZZAZIONE E RISORSE                 ': 7,\n",
       " 'DIPARTIMENTO POLITICHE FORMATIVE E BENI CULTURALI     ': 8,\n",
       " 'DIPARTIMENTO POLITICHE TERRITORIALI E AMBIENTALI      ': 9,\n",
       " 'DIPARTIMENTO PRESIDENZA AFFARI LEGISLATIVI E GIURIDICI': 10,\n",
       " 'DIPARTIMENTO SALUTE E POLITICHE SOLIDARIETA           ': 11,\n",
       " 'DIPARTIMENTO SVILUPPO ECONOMICO                       ': 12,\n",
       " 'DIREZIONE AGRICOLTURA E SVILUPPO RURALE': 13,\n",
       " \"DIREZIONE ATTIVITA' PRODUTTIVE\": 14,\n",
       " 'DIREZIONE DIFESA DEL SUOLO E PROTEZIONE CIVILE': 15,\n",
       " 'DIREZIONE DIRITTI DI CITTADINANZA E COESIONE SOCIALE': 16,\n",
       " 'DIREZIONE GENERALE BILANCIO E FINANZE                 ': 17,\n",
       " 'DIREZIONE GENERALE DIRITTO ALLA SALUTE E POLITICHE DI ': 18,\n",
       " 'DIREZIONE GENERALE POLITICHE FORMATIVE, BENI E ATTIVIT': 19,\n",
       " \"DIREZIONE GENERALE POLITICHE MOBILITA', INFRASTRUTTURE E TRASPORTO PUBBLICO LOCALE\": 20,\n",
       " 'DIREZIONE GENERALE POLITICHE TERRITORIALI E AMBIENTALI': 21,\n",
       " 'DIREZIONE GENERALE SVILUPPO ECONOMICO                 ': 22,\n",
       " 'DIREZIONE ISTRUZIONE E FORMAZIONE': 23,\n",
       " 'DIREZIONE LAVORO': 24,\n",
       " 'DIREZIONE ORGANIZZAZIONE E SISTEMI INFORMATIVI': 25,\n",
       " 'POLITICHE AMBIENTALI, ENERGIA E CAMBIAMENTI CLIMATICI': 26,\n",
       " 'UFFICI DEL GENIO CIVILE                               ': 27}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.concatenate((x_train, x_test))\n",
    "y = np.concatenate((y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34731"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([23, 3, 6376, 3, 151, 161, 3, 31775, 7, 2, 8, 1489, 4, 2, 207, 6, 2, 263, 3751, 6399, 3526, 20, 718, 8, 19, 649, 16, 8865, 4, 215, 24324, 14, 2411, 4, 257, 3462, 3, 35, 28, 1804, 25, 2969, 3]),\n",
       "       list([33, 683, 2, 206, 41, 1564, 1791, 1855, 3545, 4, 65, 28, 126, 7311, 6, 2904, 3, 106, 1719, 1792, 209, 37, 187, 2705]),\n",
       "       list([23, 3, 1697, 5, 43, 1328, 38, 466, 27, 1047, 4, 8165, 5, 22, 6, 26, 14, 48, 25, 398, 1303, 11, 394, 4105, 4, 211, 2, 5]),\n",
       "       ...,\n",
       "       list([85, 14210, 7, 455, 811, 204, 29, 26, 14, 48, 25, 435, 50, 460, 4, 58, 9, 120, 6, 202, 7, 96, 49, 4, 167, 27, 738, 4, 102, 2202, 1841, 193, 49, 3837, 3162]),\n",
       "       list([2370, 1509, 3, 129, 7, 3701, 7478, 5, 4204, 3768, 15, 307, 25, 448, 2348, 17082, 7873, 10857, 17, 6, 307, 25, 7274, 1024, 6, 484, 8, 32, 3886, 17, 3, 529, 4, 1984, 73, 20, 66, 8, 39, 1260, 4, 511, 4, 920, 6, 4, 129, 4, 5549, 606, 426, 3]),\n",
       "       list([3245, 1067, 8540, 6286, 8, 32, 49, 222, 6, 10294, 8, 32, 49, 281, 761, 1244, 10295, 2534, 8, 257, 3462, 25, 398, 4, 201, 9, 58, 9, 224, 9, 212, 9, 171, 9, 271, 393, 9, 120, 6, 202, 3, 812, 10041, 3])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def x_transformer(x_data):\n",
    "    return partial(dataset_utils.vectorize_sequences, x_data, num_words)\n",
    "\n",
    "def y_tranformer(y_data):\n",
    "    return partial(dataset_utils.to_one_hot, y_data, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dropout_model(neurons, num_words, num_classes, dropout=0.5):\n",
    "    input_l = keras.Input(shape=(num_words, ))\n",
    "    l = keras.layers.Dense(neurons, activation='relu')(input_l)\n",
    "    l = keras.layers.Dropout(dropout)(l)\n",
    "    l = keras.layers.Dense(neurons, activation='relu')(l)\n",
    "    l = keras.layers.Dropout(dropout)(l)\n",
    "    l = keras.layers.Dense(neurons, activation='relu')(l)\n",
    "    output_l = keras.layers.Dense(num_classes, activation='softmax')(l)\n",
    "    model = keras.Model(inputs=input_l, outputs=output_l)\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps = len(x_train) // batch_size + 1\n",
    "\n",
    "train_generator = dataset_utils.dataset_generator_fun(x_train, y_train, x_transformer, y_tranformer, \n",
    "                                                      batch_size, 0, len(x_train), True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 34731)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               17782784  \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 28)                14364     \n",
      "=================================================================\n",
      "Total params: 18,322,460\n",
      "Trainable params: 18,322,460\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_dropout_model(512, num_words, num_classes)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../checkpoints/weights.{epoch:02d}-{loss:.2f}.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(model_path, verbose=1, save_best_only=True, monitor='loss'),\n",
    "    keras.callbacks.ReduceLROnPlateau(patience=2, verbose=1, monitor='loss')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "955/955 [==============================] - 35s 36ms/step - loss: 0.8795 - acc: 0.7229\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.87973, saving model to ../checkpoints/weights.01-0.88.hdf5\n",
      "Epoch 2/3\n",
      "955/955 [==============================] - 34s 36ms/step - loss: 0.4417 - acc: 0.8565\n",
      "\n",
      "Epoch 00002: loss improved from 0.87973 to 0.44180, saving model to ../checkpoints/weights.02-0.44.hdf5\n",
      "Epoch 3/3\n",
      "955/955 [==============================] - 34s 35ms/step - loss: 0.3302 - acc: 0.8923\n",
      "\n",
      "Epoch 00003: loss improved from 0.44180 to 0.33017, saving model to ../checkpoints/weights.03-0.33.hdf5\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator, train_steps, 3, callbacks=train_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../checkpoints/final_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('../checkpoints/baseline.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = x_transformer(x_train[:10])()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = model.predict_on_batch(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 14,  9, 15, 16,  3, 22, 13, 16, 22])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(r, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4, 14,  9, 15, 16,  3, 22, 13, 16, 22])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/fabio/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/fabio/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = ['-', '\"', \"'\", ':', ';', '(', ')', '[', ']', '{', '}', '’', '”', '“', '``', \"''\"]\n",
    "stop_words = set(stopwords.words('italian'))\n",
    "stop_words.update(punctuation)\n",
    "\n",
    "pad_char = 0\n",
    "start_char=1\n",
    "oov_char=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/dataset/id_word_dict.json', 'r') as f:\n",
    "    id_word_dict = json.load(f)\n",
    "\n",
    "with open('../data/dataset/label_index.json', 'r') as f:\n",
    "    label_id_dict = json.load(f)\n",
    "    id_label_dict = {v: k for k, v in label_id_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_id_dict = {v:int(k) for k,v in id_word_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasnumbers(value):\n",
    "    return any(c.isdigit() for c in value)\n",
    "\n",
    "\n",
    "def tokenize_sentence(sentence, remove_stopwords=False, tokenizer=tokenizer.tokenize):\n",
    "    \"\"\"\n",
    "    Tokenize the sentence and remove stopwords if true\n",
    "    :param sentence: the sentence to be tokenized\n",
    "    :param remove_stopwords: True to remove stopwa\n",
    "    :param tokenizer:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    sentence = sentence.replace('`', ' ')\n",
    "    sentence = sentence.replace(\"'\", \" \")\n",
    "    sentence = sentence.replace(\"”\", ' ')\n",
    "    sentence = sentence.replace(\"“\", ' ')\n",
    "    words = []\n",
    "\n",
    "    for w in tokenizer(sentence):\n",
    "        if not hasnumbers(w) and len(w) > 2:\n",
    "            w = w.replace('_', '')\n",
    "            if remove_stopwords:\n",
    "                if w not in stop_words:\n",
    "                    words.append(w.lower())\n",
    "            elif w in stop_words or len(w) > 1:\n",
    "                words.append(w.lower())\n",
    "    yield words\n",
    "\n",
    "\n",
    "def sentence_to_idxs(tokenized_sentence):\n",
    "    \"\"\"\n",
    "    convert a tokenized sentence into a sequence of idx\n",
    "    :param tokenized_sentence:\n",
    "    :param max_idx:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for sample in tokenized_sentence:\n",
    "        encoded_sample = []\n",
    "        for w in sample:\n",
    "            if w in word_id_dict:\n",
    "                encoded_sample.append(word_id_dict[w])\n",
    "            else:\n",
    "                encoded_sample.append(oov_char)\n",
    "        results.append(encoded_sample)\n",
    "    return results\n",
    "\n",
    "\n",
    "def vectorize_sequences(sequences, num_words):\n",
    "    \"\"\"\n",
    "\n",
    "    :param sequences:\n",
    "    :param dimension:\n",
    "    :return: sequences encoded as indicator arrays\n",
    "    \"\"\"\n",
    "    results = np.zeros((len(sequences), num_words))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "def sentence_pipeline(sentence):\n",
    "    \"\"\"\n",
    "    :param sentence:\n",
    "    :return: the sentence into its vectorized form\n",
    "    \"\"\"\n",
    "    tokenized = list(tokenize_sentence(sentence))\n",
    "    sequences = list(sentence_to_idxs(tokenized))\n",
    "    vectorized = vectorize_sequences([sequences], num_words)\n",
    "    return vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"\n",
    "Approvazione di una seconda variante al progetto esecutivo delle opere di adeguamento di un invaso artificiale in loc. Piano S. Croce, nel Comune di Monterotondo Marittimo (GR) - ditta Piazzi s.r.l. - L.R.T. n° 64 del 5.11.2009 e s.m.i. e Regolamento n°18/r del 25.02.2010 a s.m.i - pratica n. 222.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized = sentence_pipeline(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 34731)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     7],\n",
       "       [    0,    24],\n",
       "       [    0,    25],\n",
       "       [    0,    33],\n",
       "       [    0,    41],\n",
       "       [    0,    45],\n",
       "       [    0,    67],\n",
       "       [    0,   125],\n",
       "       [    0,   166],\n",
       "       [    0,   182],\n",
       "       [    0,   247],\n",
       "       [    0,   324],\n",
       "       [    0,   333],\n",
       "       [    0,   377],\n",
       "       [    0,   466],\n",
       "       [    0,   687],\n",
       "       [    0,   708],\n",
       "       [    0,  1032],\n",
       "       [    0,  1224],\n",
       "       [    0,  1903],\n",
       "       [    0,  2282],\n",
       "       [    0,  2593],\n",
       "       [    0, 15242]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argwhere(vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(model.predict(vectorized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
