{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabio/miniconda3/envs/dl/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import *\n",
    "from sklearn import metrics\n",
    "\n",
    "from daf.datasets import atti_dataset\n",
    "from daf.utils import dataset_utils\n",
    "from daf.utils import keras_util\n",
    "import keras\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37658"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = atti_dataset.load_data(num_words=None)\n",
    "label_index_dict = atti_dataset.get_label_index()\n",
    "\n",
    "num_words = max([max(x) for x in x_train]) + 1\n",
    "num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ALTRI UFFICI': 0,\n",
       " 'AVVOCATURA REGIONALE                                  ': 1,\n",
       " 'D.G.  AVVOCATURA                                      ': 2,\n",
       " \"D.G. COMPETITIVITA' DEL SISTEMA REGIONALE E SVILUPPO D\": 3,\n",
       " 'D.G. DIRITTI DI CITTADINANZA E COESIONE SOCIALE       ': 4,\n",
       " 'D.G. ORGANIZZAZIONE                                   ': 5,\n",
       " 'D.G. ORGANIZZAZIONE E RISORSE                         ': 6,\n",
       " 'D.G. POLITICHE TERRITORIALI, AMBIENTALI E PER LA MOBIL': 7,\n",
       " 'D.G. PRESIDENZA                                       ': 8,\n",
       " 'DIPARTIMENTO BILANCIO E FINANZE                       ': 9,\n",
       " 'DIPARTIMENTO ORGANIZZAZIONE                           ': 10,\n",
       " 'DIPARTIMENTO ORGANIZZAZIONE E RISORSE                 ': 11,\n",
       " 'DIPARTIMENTO POLITICHE FORMATIVE E BENI CULTURALI     ': 12,\n",
       " 'DIPARTIMENTO POLITICHE TERRITORIALI E AMBIENTALI      ': 13,\n",
       " 'DIPARTIMENTO PRESIDENZA AFFARI LEGISLATIVI E GIURIDICI': 14,\n",
       " 'DIPARTIMENTO SALUTE E POLITICHE SOLIDARIETA           ': 15,\n",
       " 'DIPARTIMENTO SVILUPPO ECONOMICO                       ': 16,\n",
       " 'DIREZIONE AGRICOLTURA E SVILUPPO RURALE': 17,\n",
       " 'DIREZIONE AMBIENTE ED ENERGIA': 18,\n",
       " \"DIREZIONE ATTIVITA' PRODUTTIVE\": 19,\n",
       " 'DIREZIONE DIFESA DEL SUOLO E PROTEZIONE CIVILE': 20,\n",
       " 'DIREZIONE DIRITTI DI CITTADINANZA E COESIONE SOCIALE': 21,\n",
       " 'DIREZIONE GENERALE BILANCIO E FINANZE                 ': 22,\n",
       " 'DIREZIONE GENERALE DIREZIONE GENERALE DELLA GIUNTA REGIONALE': 23,\n",
       " 'DIREZIONE GENERALE DIRITTO ALLA SALUTE E POLITICHE DI ': 24,\n",
       " 'DIREZIONE GENERALE ORGANIZZAZIONE E SISTEMA INFORMATIV': 25,\n",
       " 'DIREZIONE GENERALE POLITICHE FORMATIVE, BENI E ATTIVIT': 26,\n",
       " 'DIREZIONE GENERALE POLITICHE TERRITORIALI E AMBIENTALI': 27,\n",
       " 'DIREZIONE GENERALE SVILUPPO ECONOMICO                 ': 28,\n",
       " 'DIREZIONE ISTRUZIONE E FORMAZIONE': 29,\n",
       " 'DIREZIONE LAVORO': 30,\n",
       " 'DIREZIONE ORGANIZZAZIONE E SISTEMI INFORMATIVI': 31,\n",
       " \"DIREZIONE POLITICHE MOBILITA', INFRASTRUTTURE E TRASPORTO PUBBLICO LOCALE\": 32,\n",
       " 'POLITICHE AMBIENTALI, ENERGIA E CAMBIAMENTI CLIMATICI': 33,\n",
       " 'UFFICI DEL GENIO CIVILE                               ': 34}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.concatenate((x_train, x_test))\n",
    "y = np.concatenate((y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37658"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([398, 1536, 6, 615, 3, 13, 484, 7, 148, 4, 676, 8, 68, 698, 23, 633, 4, 1498, 1463, 105, 1499, 4, 158, 16, 3, 100, 44, 498, 3, 1881, 50, 237, 3]),\n",
       "       list([125, 18, 26, 2, 7, 261, 157, 6, 611, 34, 10, 1477, 8271, 258, 35, 414, 4, 2, 2]),\n",
       "       list([236, 384, 20, 412, 364, 297, 9, 11, 1196, 5, 60, 755, 49, 56, 1367, 43, 160, 24, 319, 756, 3, 172, 24, 512, 8, 37, 597, 4, 1312, 56, 1453, 36, 48, 92, 499, 491, 285, 11, 20167, 3]),\n",
       "       ...,\n",
       "       list([1045, 4, 257, 30, 13, 17, 16, 5, 68, 771, 163, 95, 23, 1036, 18, 1044, 3, 5, 13, 307, 16, 5, 771, 53, 284, 386, 228, 99, 6, 228, 648, 15, 1028, 23, 410, 1575, 30, 37, 4555, 309, 3, 26, 4, 41, 8, 37, 685, 958, 336, 3]),\n",
       "       list([273, 21, 379, 171, 21, 380, 631, 8, 31, 75, 4, 748, 428, 942, 228, 46, 32, 14, 2, 12, 3, 135, 15, 202, 1159, 162, 20, 3]),\n",
       "       list([77, 2765, 763, 6, 77, 1841, 2520, 9, 736, 5, 342, 9, 70, 9, 26, 6, 34, 7, 44, 971, 15, 666, 27, 850, 3, 53, 3, 184, 128, 3, 4, 59, 8, 76, 72, 4, 2428, 337, 43, 25, 4, 59, 5, 411, 4073, 3])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def x_transformer(x_data):\n",
    "    return partial(dataset_utils.vectorize_sequences, x_data, num_words)\n",
    "\n",
    "def y_tranformer(y_data):\n",
    "    return partial(dataset_utils.to_one_hot, y_data, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = len(label_index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dropout_model(neurons, num_words, num_classes, dropout=0.5):\n",
    "    input_l = keras.Input(shape=(num_words, ))\n",
    "    l = keras.layers.Dense(neurons, activation='relu')(input_l)\n",
    "    l = keras.layers.Dropout(dropout)(l)\n",
    "    l = keras.layers.Dense(neurons, activation='relu')(l)\n",
    "    l = keras.layers.Dropout(dropout)(l)\n",
    "    l = keras.layers.Dense(neurons, activation='relu')(l)\n",
    "    output_l = keras.layers.Dense(num_classes, activation='softmax')(l)\n",
    "    model = keras.Model(inputs=input_l, outputs=output_l)\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps = len(x_train) // batch_size + 1\n",
    "\n",
    "train_generator = dataset_utils.dataset_generator_fun(x_train, y_train, x_transformer, y_tranformer, \n",
    "                                                      batch_size, 0, len(x_train), True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 34731)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               17782784  \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 28)                14364     \n",
      "=================================================================\n",
      "Total params: 18,322,460\n",
      "Trainable params: 18,322,460\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_dropout_model(512, num_words, num_classes)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../checkpoints/weights.{epoch:02d}-{loss:.2f}.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(model_path, verbose=1, save_best_only=True, monitor='loss'),\n",
    "    keras.callbacks.ReduceLROnPlateau(patience=2, verbose=1, monitor='loss')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "955/955 [==============================] - 35s 36ms/step - loss: 0.8795 - acc: 0.7229\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.87973, saving model to ../checkpoints/weights.01-0.88.hdf5\n",
      "Epoch 2/3\n",
      "955/955 [==============================] - 34s 36ms/step - loss: 0.4417 - acc: 0.8565\n",
      "\n",
      "Epoch 00002: loss improved from 0.87973 to 0.44180, saving model to ../checkpoints/weights.02-0.44.hdf5\n",
      "Epoch 3/3\n",
      "955/955 [==============================] - 34s 35ms/step - loss: 0.3302 - acc: 0.8923\n",
      "\n",
      "Epoch 00003: loss improved from 0.44180 to 0.33017, saving model to ../checkpoints/weights.03-0.33.hdf5\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator, train_steps, 3, callbacks=train_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../checkpoints/final_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('../checkpoints/baseline.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = x_transformer(x_train[:10])()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = model.predict_on_batch(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4,  3, 26, 18, 24,  3,  3, 16, 22, 13])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(r, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4,  3, 26, 18, 24,  3,  3, 16,  8, 13])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/fabio/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/fabio/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = ['-', '\"', \"'\", ':', ';', '(', ')', '[', ']', '{', '}', '’', '”', '“', '``', \"''\"]\n",
    "stop_words = set(stopwords.words('italian'))\n",
    "stop_words.update(punctuation)\n",
    "\n",
    "pad_char = 0\n",
    "start_char=1\n",
    "oov_char=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/dataset/id_word_dict.json', 'r') as f:\n",
    "    id_word_dict = json.load(f)\n",
    "\n",
    "with open('../data/dataset/label_index.json', 'r') as f:\n",
    "    label_id_dict = json.load(f)\n",
    "    id_label_dict = {v: k for k, v in label_id_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_id_dict = {v:int(k) for k,v in id_word_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasnumbers(value):\n",
    "    return any(c.isdigit() for c in value)\n",
    "\n",
    "\n",
    "def tokenize_sentence(sentence, remove_stopwords=False, tokenizer=tokenizer.tokenize):\n",
    "    \"\"\"\n",
    "    Tokenize the sentence and remove stopwords if true\n",
    "    :param sentence: the sentence to be tokenized\n",
    "    :param remove_stopwords: True to remove stopwa\n",
    "    :param tokenizer:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    sentence = sentence.replace('`', ' ')\n",
    "    sentence = sentence.replace(\"'\", \" \")\n",
    "    sentence = sentence.replace(\"”\", ' ')\n",
    "    sentence = sentence.replace(\"“\", ' ')\n",
    "    words = []\n",
    "\n",
    "    for w in tokenizer(sentence):\n",
    "        if not hasnumbers(w) and len(w) > 2:\n",
    "            w = w.replace('_', '')\n",
    "            if remove_stopwords:\n",
    "                if w not in stop_words:\n",
    "                    words.append(w.lower())\n",
    "            elif w in stop_words or len(w) > 1:\n",
    "                words.append(w.lower())\n",
    "    yield words\n",
    "\n",
    "\n",
    "def sentence_to_idxs(tokenized_sentence):\n",
    "    \"\"\"\n",
    "    convert a tokenized sentence into a sequence of idx\n",
    "    :param tokenized_sentence:\n",
    "    :param max_idx:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for sample in tokenized_sentence:\n",
    "        encoded_sample = []\n",
    "        for w in sample:\n",
    "            if w in word_id_dict:\n",
    "                encoded_sample.append(word_id_dict[w])\n",
    "            else:\n",
    "                encoded_sample.append(oov_char)\n",
    "        results.append(encoded_sample)\n",
    "    return results\n",
    "\n",
    "\n",
    "def vectorize_sequences(sequences, num_words):\n",
    "    \"\"\"\n",
    "\n",
    "    :param sequences:\n",
    "    :param dimension:\n",
    "    :return: sequences encoded as indicator arrays\n",
    "    \"\"\"\n",
    "    results = np.zeros((len(sequences), num_words))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "def sentence_pipeline(sentence):\n",
    "    \"\"\"\n",
    "    :param sentence:\n",
    "    :return: the sentence into its vectorized form\n",
    "    \"\"\"\n",
    "    tokenized = list(tokenize_sentence(sentence))\n",
    "    sequences = list(sentence_to_idxs(tokenized))\n",
    "    vectorized = vectorize_sequences([sequences], num_words)\n",
    "    return vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"\n",
    "Approvazione di una seconda variante al progetto esecutivo delle opere di adeguamento di un invaso artificiale in loc. Piano S. Croce, nel Comune di Monterotondo Marittimo (GR) - ditta Piazzi s.r.l. - L.R.T. n° 64 del 5.11.2009 e s.m.i. e Regolamento n°18/r del 25.02.2010 a s.m.i - pratica n. 222.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized = sentence_pipeline(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 37658)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     7],\n",
       "       [    0,    23],\n",
       "       [    0,    25],\n",
       "       [    0,    28],\n",
       "       [    0,    40],\n",
       "       [    0,    43],\n",
       "       [    0,    58],\n",
       "       [    0,   139],\n",
       "       [    0,   190],\n",
       "       [    0,   200],\n",
       "       [    0,   249],\n",
       "       [    0,   265],\n",
       "       [    0,   289],\n",
       "       [    0,   334],\n",
       "       [    0,   437],\n",
       "       [    0,   673],\n",
       "       [    0,   682],\n",
       "       [    0,   873],\n",
       "       [    0,  1311],\n",
       "       [    0,  1877],\n",
       "       [    0,  2499],\n",
       "       [    0,  2759],\n",
       "       [    0, 15668]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argwhere(vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(model.predict(vectorized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
