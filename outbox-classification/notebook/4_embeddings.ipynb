{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabio/miniconda3/envs/dl/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import *\n",
    "from sklearn import metrics\n",
    "\n",
    "from daf.datasets import atti_dataset\n",
    "from daf.utils import dataset_utils\n",
    "from daf.utils import keras_util\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52396"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = atti_dataset.load_data(num_words=None)\n",
    "label_index_dict = atti_dataset.get_label_index()\n",
    "\n",
    "num_words = max([max(x) for x in x_train]) + 1\n",
    "num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 34 classes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ALTRI UFFICI': 0,\n",
       " 'AVVOCATURA REGIONALE                                  ': 1,\n",
       " 'D.G.  AVVOCATURA                                      ': 2,\n",
       " \"D.G. COMPETITIVITA' DEL SISTEMA REGIONALE E SVILUPPO D\": 3,\n",
       " 'D.G. PRESIDENZA                                       ': 4,\n",
       " 'DIPARTIMENTO BILANCIO E FINANZE                       ': 5,\n",
       " 'DIPARTIMENTO ORGANIZZAZIONE                           ': 6,\n",
       " 'DIPARTIMENTO ORGANIZZAZIONE E RISORSE                 ': 7,\n",
       " 'DIPARTIMENTO POLITICHE FORMATIVE E BENI CULTURALI     ': 8,\n",
       " 'DIPARTIMENTO POLITICHE TERRITORIALI E AMBIENTALI      ': 9,\n",
       " 'DIPARTIMENTO PRESIDENZA AFFARI LEGISLATIVI E GIURIDICI': 10,\n",
       " 'DIPARTIMENTO SALUTE E POLITICHE SOLIDARIETA           ': 11,\n",
       " 'DIPARTIMENTO SVILUPPO ECONOMICO                       ': 12,\n",
       " 'DIREZIONE AFFARI LEGISLATIVI, GIURIDICI ED ISTITUZIONALI': 13,\n",
       " 'DIREZIONE AGRICOLTURA E SVILUPPO RURALE': 14,\n",
       " \"DIREZIONE ATTIVITA' PRODUTTIVE\": 15,\n",
       " 'DIREZIONE CULTURA E RICERCA': 16,\n",
       " 'DIREZIONE DIFESA DEL SUOLO E PROTEZIONE CIVILE': 17,\n",
       " 'DIREZIONE DIRITTI DI CITTADINANZA E COESIONE SOCIALE': 18,\n",
       " 'DIREZIONE GENERALE BILANCIO E FINANZE                 ': 19,\n",
       " 'DIREZIONE GENERALE DIREZIONE GENERALE DELLA GIUNTA REGIONALE': 20,\n",
       " 'DIREZIONE GENERALE DIRITTO ALLA SALUTE E POLITICHE DI ': 21,\n",
       " 'DIREZIONE GENERALE POLITICHE FORMATIVE, BENI E ATTIVIT': 22,\n",
       " \"DIREZIONE GENERALE POLITICHE MOBILITA', INFRASTRUTTURE E TRASPORTO PUBBLICO LOCALE\": 23,\n",
       " 'DIREZIONE GENERALE POLITICHE TERRITORIALI E AMBIENTALI': 24,\n",
       " 'DIREZIONE GENERALE SVILUPPO ECONOMICO                 ': 25,\n",
       " 'DIREZIONE ISTRUZIONE E FORMAZIONE': 26,\n",
       " 'DIREZIONE LAVORO': 27,\n",
       " 'DIREZIONE ORGANIZZAZIONE E SISTEMI INFORMATIVI': 28,\n",
       " 'DIREZIONE PROGRAMMAZIONE E BILANCIO': 29,\n",
       " 'DIREZIONE URBANISTICA, CASA E POLITICHE ABITATIVE': 30,\n",
       " 'POLITICHE AMBIENTALI, ENERGIA E CAMBIAMENTI CLIMATICI': 31,\n",
       " 'SEGRETERIE ORGANI POLITICI UFFICIO DI GABINETTO': 32,\n",
       " 'UFFICI DEL GENIO CIVILE                               ': 33}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Total of {} classes'.format(len(label_index_dict)))\n",
    "label_index_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "We need to create the function that transform the x and y.\n",
    "In this case we need to:\n",
    "- x: pad the sequences\n",
    "- y: one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen = max([len(x) for x in x_train])\n",
    "maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def x_transformer(x_data):\n",
    "    return partial(tf.keras.preprocessing.sequence.pad_sequences, x_data, maxlen)\n",
    "\n",
    "def y_tranformer(y_data):\n",
    "    return partial(dataset_utils.to_one_hot, y_data, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num classes 34\n",
      "training size 117777, validation size 29444\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_classes = len(set(y_train))\n",
    "train_val_split = math.ceil(len(x_train) * 0.8)\n",
    "print('num classes {}'.format(num_classes))\n",
    "print('training size {}, validation size {}'.format(train_val_split, len(x_train) - train_val_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = dataset_utils.dataset_generator_fun(x_train, y_train, x_transformer, y_tranformer, \n",
    "                                                      batch_size, 0, train_val_split, True)\n",
    "\n",
    "val_generator = dataset_utils.dataset_generator_fun(x_train, y_train, x_transformer, y_tranformer,\n",
    "                                                    batch_size, train_val_split, len(x_train), False)\n",
    "\n",
    "train_steps = train_val_split // batch_size + 1\n",
    "val_steps = (len(x_train) - train_val_split) // batch_size + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Effects of Word Embeddings\n",
    "\n",
    "Before using a word embedding as a layer in our network let evaluate the effect of embeddings by training a simple classifier that has it as only layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embed_model(num_words, num_classes, embed_size):\n",
    "    keras_util.new_session()\n",
    "    input_l = tf.keras.Input(shape=(maxlen,), dtype='int32')\n",
    "    embed_l = tf.keras.layers.Embedding(input_dim=num_words, output_dim=embed_size, name='embed')(input_l)\n",
    "    l = tf.keras.layers.Flatten()(embed_l)\n",
    "    output_l = tf.keras.layers.Dense(num_classes, activation='softmax')(l)\n",
    "    model = tf.keras.Model(inputs=input_l, outputs=output_l)\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 101)               0         \n",
      "_________________________________________________________________\n",
      "embed (Embedding)            (None, 101, 64)           3353344   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6464)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 34)                219810    \n",
      "=================================================================\n",
      "Total params: 3,573,154\n",
      "Trainable params: 3,573,154\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_model = build_embed_model(num_words, num_classes, 64)\n",
    "embed_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=\"logs/{}_{}\".format('only_embedding', '64')),\n",
    "    tf.keras.callbacks.EarlyStopping(patience=3, verbose=1),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(patience=3)\n",
    "    ]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "921/921 [==============================] - 23s 25ms/step - loss: 2.7280 - acc: 0.2964 - val_loss: 2.1665 - val_acc: 0.3578\n",
      "Epoch 2/10\n",
      "921/921 [==============================] - 22s 24ms/step - loss: 1.5488 - acc: 0.5621 - val_loss: 1.4319 - val_acc: 0.5725\n",
      "Epoch 3/10\n",
      "921/921 [==============================] - 22s 24ms/step - loss: 1.0818 - acc: 0.6799 - val_loss: 1.2066 - val_acc: 0.6197\n",
      "Epoch 4/10\n",
      "921/921 [==============================] - 23s 25ms/step - loss: 0.8250 - acc: 0.7466 - val_loss: 1.0912 - val_acc: 0.6480\n",
      "Epoch 5/10\n",
      "921/921 [==============================] - 23s 25ms/step - loss: 0.6706 - acc: 0.7943 - val_loss: 1.0621 - val_acc: 0.6592\n",
      "Epoch 6/10\n",
      "921/921 [==============================] - 23s 25ms/step - loss: 0.5568 - acc: 0.8289 - val_loss: 1.0408 - val_acc: 0.6682\n",
      "Epoch 7/10\n",
      "921/921 [==============================] - 23s 25ms/step - loss: 0.4680 - acc: 0.8584 - val_loss: 1.0343 - val_acc: 0.6773\n",
      "Epoch 8/10\n",
      "921/921 [==============================] - 23s 25ms/step - loss: 0.3926 - acc: 0.8829 - val_loss: 1.0419 - val_acc: 0.6828\n",
      "Epoch 9/10\n",
      "921/921 [==============================] - 24s 26ms/step - loss: 0.3318 - acc: 0.9014 - val_loss: 1.0606 - val_acc: 0.6835\n",
      "Epoch 10/10\n",
      "921/921 [==============================] - 24s 26ms/step - loss: 0.2859 - acc: 0.9158 - val_loss: 1.0841 - val_acc: 0.6829\n",
      "Epoch 00010: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x7ff8401c9a20>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_model.fit_generator(train_generator, steps_per_epoch=train_steps, epochs=10, \n",
    "                          validation_data=val_generator, validation_steps=val_steps, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 101)               0         \n",
      "_________________________________________________________________\n",
      "embed (Embedding)            (None, 101, 128)          6706688   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 12928)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 34)                439586    \n",
      "=================================================================\n",
      "Total params: 7,146,274\n",
      "Trainable params: 7,146,274\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_model = build_embed_model(num_words, num_classes, 128)\n",
    "embed_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=\"logs/{}_{}\".format('only_embedding', '128')),\n",
    "    tf.keras.callbacks.EarlyStopping(patience=3, verbose=1),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(patience=3)\n",
    "    ]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "921/921 [==============================] - 25s 27ms/step - loss: 5.9699 - acc: 0.2890 - val_loss: 5.6423 - val_acc: 0.4075\n",
      "Epoch 2/10\n",
      "921/921 [==============================] - 24s 26ms/step - loss: 5.2051 - acc: 0.5094 - val_loss: 5.1882 - val_acc: 0.5110\n",
      "Epoch 3/10\n",
      "921/921 [==============================] - 24s 26ms/step - loss: 4.3668 - acc: 0.5912 - val_loss: 4.1865 - val_acc: 0.5507\n",
      "Epoch 4/10\n",
      "921/921 [==============================] - 24s 26ms/step - loss: 2.9886 - acc: 0.6711 - val_loss: 3.1103 - val_acc: 0.5881\n",
      "Epoch 5/10\n",
      "921/921 [==============================] - 24s 26ms/step - loss: 1.5582 - acc: 0.7453 - val_loss: 1.2185 - val_acc: 0.6299\n",
      "Epoch 6/10\n",
      "921/921 [==============================] - 24s 26ms/step - loss: 0.5988 - acc: 0.8212 - val_loss: 1.1077 - val_acc: 0.6607\n",
      "Epoch 7/10\n",
      "921/921 [==============================] - 24s 26ms/step - loss: 0.4589 - acc: 0.8623 - val_loss: 1.1041 - val_acc: 0.6672\n",
      "Epoch 8/10\n",
      "921/921 [==============================] - 24s 26ms/step - loss: 0.3596 - acc: 0.8933 - val_loss: 1.1224 - val_acc: 0.6746\n",
      "Epoch 9/10\n",
      "921/921 [==============================] - 24s 26ms/step - loss: 0.2956 - acc: 0.9127 - val_loss: 1.1317 - val_acc: 0.6789\n",
      "Epoch 10/10\n",
      "921/921 [==============================] - 24s 26ms/step - loss: 0.2432 - acc: 0.9292 - val_loss: 1.2051 - val_acc: 0.6733\n",
      "Epoch 00010: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x7ff8436328d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_model.fit_generator(train_generator, steps_per_epoch=train_steps, epochs=10, \n",
    "                          validation_data=val_generator, validation_steps=val_steps, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add an embedding layer to our previous best classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_embedding(neurons, num_words, num_class, embed_size, maxlen, dropout):\n",
    "    keras_util.new_session()\n",
    "    input_l = tf.keras.Input(shape=(maxlen,), dtype='int32')\n",
    "    embed_l = tf.keras.layers.Embedding(input_dim=num_words, output_dim=embed_size, name='embed')(input_l)\n",
    "    l = tf.keras.layers.Flatten()(embed_l)\n",
    "    l = tf.keras.layers.Dense(neurons, activation='relu')(l)\n",
    "    l = tf.keras.layers.Dropout(dropout)(l)\n",
    "    l = tf.keras.layers.Dense(neurons, activation='relu')(l)\n",
    "    l = tf.keras.layers.Dropout(dropout)(l)\n",
    "    output_l = tf.keras.layers.Dense(num_classes, activation='softmax')(l)\n",
    "    model = tf.keras.Model(inputs=input_l, outputs=output_l)\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(embeds, epochs):\n",
    "    histories = {}\n",
    "    for embed_size in embeds:\n",
    "        print('*** Network with embedding {} ***'.format(embed_size))\n",
    "        model = build_model_embedding(128, num_words, num_classes, embed_size, maxlen, 0.5)\n",
    "        print(model.summary())\n",
    "        \n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.TensorBoard(log_dir=\"logs/embeds_{}\".format(embed_size)),\n",
    "            tf.keras.callbacks.EarlyStopping(patience=3, verbose=1),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(patience=3)\n",
    "            ]  \n",
    "        \n",
    "        history = model.fit_generator(train_generator, steps_per_epoch=train_steps, epochs=epochs, \n",
    "                      validation_data=val_generator, validation_steps=val_steps, callbacks=callbacks)\n",
    "\n",
    "        histories['embed_{}'.format(embed_size)] = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "845"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Network with embedding 64 ***\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 101)               0         \n",
      "_________________________________________________________________\n",
      "embed (Embedding)            (None, 101, 64)           3353344   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6464)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               827520    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 34)                4386      \n",
      "=================================================================\n",
      "Total params: 4,201,762\n",
      "Trainable params: 4,201,762\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "921/921 [==============================] - 23s 25ms/step - loss: 2.6117 - acc: 0.2468 - val_loss: 1.9722 - val_acc: 0.4071\n",
      "Epoch 2/10\n",
      "921/921 [==============================] - 23s 25ms/step - loss: 1.5494 - acc: 0.5108 - val_loss: 1.5251 - val_acc: 0.5270\n",
      "Epoch 3/10\n",
      "921/921 [==============================] - 23s 25ms/step - loss: 1.2034 - acc: 0.6077 - val_loss: 1.3524 - val_acc: 0.5747\n",
      "Epoch 4/10\n",
      "921/921 [==============================] - 23s 25ms/step - loss: 1.0285 - acc: 0.6650 - val_loss: 1.3538 - val_acc: 0.5885\n",
      "Epoch 5/10\n",
      "921/921 [==============================] - 23s 25ms/step - loss: 0.8898 - acc: 0.7117 - val_loss: 1.3401 - val_acc: 0.6034\n",
      "Epoch 6/10\n",
      "921/921 [==============================] - 23s 25ms/step - loss: 0.7767 - acc: 0.7470 - val_loss: 1.3135 - val_acc: 0.6155\n",
      "Epoch 7/10\n",
      "921/921 [==============================] - 23s 25ms/step - loss: 0.6823 - acc: 0.7785 - val_loss: 1.2924 - val_acc: 0.6299\n",
      "Epoch 8/10\n",
      "921/921 [==============================] - 23s 25ms/step - loss: 0.6063 - acc: 0.8040 - val_loss: 1.3731 - val_acc: 0.6232\n",
      "Epoch 9/10\n",
      "921/921 [==============================] - 23s 25ms/step - loss: 0.5353 - acc: 0.8273 - val_loss: 1.3800 - val_acc: 0.6353\n",
      "Epoch 10/10\n",
      "921/921 [==============================] - 23s 25ms/step - loss: 0.4746 - acc: 0.8486 - val_loss: 1.4178 - val_acc: 0.6408\n",
      "Epoch 00010: early stopping\n",
      "*** Network with embedding 128 ***\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 101)               0         \n",
      "_________________________________________________________________\n",
      "embed (Embedding)            (None, 101, 128)          6706688   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 12928)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               1654912   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 34)                4386      \n",
      "=================================================================\n",
      "Total params: 8,382,498\n",
      "Trainable params: 8,382,498\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "921/921 [==============================] - 25s 27ms/step - loss: 2.4751 - acc: 0.2865 - val_loss: 1.8073 - val_acc: 0.4476\n",
      "Epoch 2/10\n",
      "921/921 [==============================] - 25s 27ms/step - loss: 1.4290 - acc: 0.5433 - val_loss: 1.4383 - val_acc: 0.5470\n",
      "Epoch 3/10\n",
      "921/921 [==============================] - 25s 27ms/step - loss: 1.0727 - acc: 0.6483 - val_loss: 1.3263 - val_acc: 0.5846\n",
      "Epoch 4/10\n",
      "921/921 [==============================] - 26s 29ms/step - loss: 0.8766 - acc: 0.7099 - val_loss: 1.2200 - val_acc: 0.6210\n",
      "Epoch 5/10\n",
      "921/921 [==============================] - 26s 28ms/step - loss: 0.7175 - acc: 0.7642 - val_loss: 1.2646 - val_acc: 0.6288\n",
      "Epoch 6/10\n",
      "921/921 [==============================] - 26s 28ms/step - loss: 0.5980 - acc: 0.8054 - val_loss: 1.2702 - val_acc: 0.6426\n",
      "Epoch 7/10\n",
      "921/921 [==============================] - 25s 27ms/step - loss: 0.5015 - acc: 0.8377 - val_loss: 1.2510 - val_acc: 0.6539\n",
      "Epoch 00007: early stopping\n"
     ]
    }
   ],
   "source": [
    "embeds = [64, 128]\n",
    "\n",
    "histories = train(embeds, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the there is no improvement in using an embedding layer, but we have milion of parameters with respect to thousand of observations. In the next part we are going to investigate the usage of a pretrained embedding model.\n",
    "\n",
    "### Use Pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_word_dict = atti_dataset.get_word_index()\n",
    "\n",
    "import gensim\n",
    "\n",
    "word2vec_model = gensim.models.Word2Vec.load('../data/dataset/atti.word2vec')\n",
    "\n",
    "embed_size = word2vec_model.vector_size\n",
    "\n",
    "counter_not_present =0\n",
    "embed_matrix = np.zeros((num_words, embed_size))\n",
    "for i, word in id_word_dict.items():\n",
    "    if word in word2vec_model:\n",
    "        vect = word2vec_model[word]\n",
    "        embed_matrix[int(i)] = vect\n",
    "    else:\n",
    "        counter_not_present +=1\n",
    "\n",
    "counter_not_present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_embedding(neurons, num_words, num_class, embed_size, maxlen, dropout):\n",
    "    keras_util.new_session()\n",
    "    input_l = tf.keras.Input(shape=(maxlen,), dtype='int32')\n",
    "    embed_l = tf.keras.layers.Embedding(input_dim=num_words, output_dim=embed_size, \n",
    "                                        weights=[embed_matrix], trainable=False, name='embed')(input_l)\n",
    "    l = tf.keras.layers.Flatten()(embed_l)\n",
    "    l = tf.keras.layers.BatchNormalization()(l)\n",
    "    l = tf.keras.layers.Dense(neurons, activation='relu')(l)\n",
    "    l = tf.keras.layers.BatchNormalization()(l)\n",
    "    l = tf.keras.layers.Dropout(dropout)(l)\n",
    "    l = tf.keras.layers.Dense(neurons, activation='relu')(l)\n",
    "    output_l = tf.keras.layers.Dense(num_classes, activation='softmax')(l)\n",
    "    model = tf.keras.Model(inputs=input_l, outputs=output_l)\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 101)               0         \n",
      "_________________________________________________________________\n",
      "embed (Embedding)            (None, 101, 100)          5239600   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 10100)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 10100)             40400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               2585856   \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 34)                8738      \n",
      "=================================================================\n",
      "Total params: 7,941,410\n",
      "Trainable params: 2,681,098\n",
      "Non-trainable params: 5,260,312\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model_embedding(256, num_words, num_classes, embed_size, maxlen, 0.1)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "921/921 [==============================] - 24s 26ms/step - loss: 1.5553 - acc: 0.5372 - val_loss: 1.5522 - val_acc: 0.5254\n",
      "Epoch 2/10\n",
      "921/921 [==============================] - 24s 26ms/step - loss: 1.1123 - acc: 0.6609 - val_loss: 1.4825 - val_acc: 0.5535\n",
      "Epoch 3/10\n",
      "921/921 [==============================] - 24s 26ms/step - loss: 0.9184 - acc: 0.7169 - val_loss: 1.4684 - val_acc: 0.5712\n",
      "Epoch 4/10\n",
      "921/921 [==============================] - 24s 27ms/step - loss: 0.7892 - acc: 0.7547 - val_loss: 1.4674 - val_acc: 0.5737\n",
      "Epoch 5/10\n",
      "921/921 [==============================] - 24s 26ms/step - loss: 0.6923 - acc: 0.7841 - val_loss: 1.5101 - val_acc: 0.5760\n",
      "Epoch 6/10\n",
      "921/921 [==============================] - 23s 25ms/step - loss: 0.6183 - acc: 0.8054 - val_loss: 1.5461 - val_acc: 0.5823\n",
      "Epoch 7/10\n",
      "921/921 [==============================] - 24s 26ms/step - loss: 0.5539 - acc: 0.8236 - val_loss: 1.5848 - val_acc: 0.5790\n",
      "Epoch 00007: early stopping\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=\"logs/pre_local_trained_embeds_{}\".format(embed_size)),\n",
    "    tf.keras.callbacks.EarlyStopping(patience=3, verbose=1),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(patience=3)\n",
    "    ]  \n",
    "\n",
    "history = model.fit_generator(train_generator, steps_per_epoch=train_steps, epochs=10, \n",
    "              validation_data=val_generator, validation_steps=val_steps, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Italian Pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_path = '../../utils/cc.it.300.vec/data'\n",
    "fasttext_model = KeyedVectors.load_word2vec_format(embed_path, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do some queries on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_model.most_similar('approvazione')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the words dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 300\n",
    "\n",
    "embed_matrix = np.zeros((num_words, embed_size))\n",
    "for i, word in id_word_dict.items():\n",
    "    if word in fasttext_model.vocab:\n",
    "        vect = fasttext_model.get_vector(word)\n",
    "        embed_matrix[int(i)] = vect\n",
    "    else:\n",
    "        counter_not_present +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_embedding(neurons, num_words, num_class, embed_size, maxlen, dropout):\n",
    "    keras_util.new_session()\n",
    "    input_l = tf.keras.Input(shape=(maxlen,), dtype='int32')\n",
    "    embed_l = tf.keras.layers.Embedding(input_dim=num_words, output_dim=embed_size, \n",
    "                                        weights=[embed_matrix], trainable=False, name='embed')(input_l)\n",
    "    l = tf.keras.layers.Flatten()(embed_l)\n",
    "    l = tf.keras.layers.Dense(neurons, activation='relu')(l)\n",
    "    l = tf.keras.layers.Dropout(dropout)(l)\n",
    "    l = tf.keras.layers.Dense(neurons, activation='relu')(l)\n",
    "    l = tf.keras.layers.Dropout(dropout)(l)\n",
    "    l = tf.keras.layers.Dense(neurons, activation='relu')(l)\n",
    "    l = tf.keras.layers.Dropout(dropout)(l)\n",
    "    output_l = tf.keras.layers.Dense(num_classes, activation='softmax')(l)\n",
    "    model = tf.keras.Model(inputs=input_l, outputs=output_l)\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = build_model_embedding(512, num_words, num_classes, embed_size, maxlen, 0.01)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=\"logs/pre_trained_embeds_{}\".format(embed_size)),\n",
    "    tf.keras.callbacks.EarlyStopping(patience=3, verbose=1),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(patience=3)\n",
    "    ]  \n",
    "\n",
    "history = model.fit_generator(train_generator, steps_per_epoch=train_steps, epochs=10, \n",
    "              validation_data=val_generator, validation_steps=val_steps, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the final model\n",
    "\n",
    "the best model is obtained embedding of size 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps = len(x_train) // batch_size + 1\n",
    "\n",
    "train_generator = dataset_utils.dataset_generator_fun(x_train, y_train, x_transformer, y_tranformer, \n",
    "                                                      batch_size, 0, len(x_train), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_embed_model(num_words, num_classes, 64)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(train_generator, train_steps, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_v = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "y_test_v = dataset_utils.to_one_hot(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(x_test_v, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array([np.argmax(x) for x in test_predictions])\n",
    "precision, recall, fscore, _ = metrics.precision_recall_fscore_support(y_test,predictions, average='weighted')\n",
    "accuracy = metrics.accuracy_score(y_test, predictions)\n",
    "\n",
    "auc_score = metrics.roc_auc_score(y_test_v, test_predictions, average='weighted')\n",
    "\n",
    "print('accuracy ', accuracy)\n",
    "print('precision ', precision)\n",
    "print('recall ', recall)\n",
    "print('f-measure ', fscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report the classification result for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = metrics.confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20,20)\n",
    "plot_confusion_matrix(conf_matrix, classes=label_index_dict,\n",
    "                      title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
